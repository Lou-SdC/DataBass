{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07acfd45",
   "metadata": {},
   "source": [
    "# DataBass Project\n",
    "\n",
    "Get the notes of the bass line from an audio file with multiple instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bf31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Récupérer WORKING_DIR\n",
    "WORKING_DIR = os.getenv(\"WORKING_DIR\")\n",
    "sys.path.append(WORKING_DIR)\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from databass.utils.get_note_baseline import get_pic_frequency, get_note, plot_fft\n",
    "from databass.preprocess.filter import frequencies_filter, plot_filtered_vs_original\n",
    "from databass.preprocess.spectrograms import generate_mel_spectrogram, plot_mel_spectrogram\n",
    "from databass.models.conv2D import conv2D_predict_note\n",
    "\n",
    "from databass.utils import bass_notes\n",
    "\n",
    "from databass.models.conv2D import create_model, preprocess_for_conv2D\n",
    "\n",
    "from databass.extract.bass_extract import extract_bass_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fd036",
   "metadata": {},
   "source": [
    "We use the librosa library, which contains useful tools to load and process audio files.\n",
    "\n",
    "First, lets extract the waveform and sample rate of one audio file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path_DataBass + '/raw_data/Bass monophon/Samples/Chorus/B11-28100-3311-00625.wav'\n",
    "y, sr = librosa.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966cc3a",
   "metadata": {},
   "source": [
    "The waveform (y) corresponds to the amplitude of the sound signal over time (well, over frames here). The sampling rate (sr) corresponds to how many times the signal was recorded per second (how many frames per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395454c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[10000:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = librosa.pyin(y, fmin=20, fmax=400)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = pd.read_csv(path_DataBass + '/notebooks/lou/table_correspondance_notes_basse.csv')\n",
    "print(df_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_frequency, magnitude, frequencies = get_pic_frequency(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft(magnitude, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_note(pic_frequency, df_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_filtered = frequencies_filter(y, sr)\n",
    "\n",
    "plot_filtered_vs_original(y, y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352527d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mel_spec = generate_mel_spectrogram(y, sr, normalize=False, target_shape=(128,128))\n",
    "print(\"Shape du Mel-spectrogramme:\", mel_spec.shape)  # (128, time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mel_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a027b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mel_spectrogram(mel_spec, sr, y=y, target_shape=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = pd.read_csv(path_DataBass + '/data/preprocessed/chorus_bass_list.csv')\n",
    "df_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = 'B31-28100-3312-01561.wav'\n",
    "label1_name = label1.replace('.wav', '')\n",
    "print(label1_name)\n",
    "df_targets[df_targets['fileID'] == label1_name]['note_name'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = path_DataBass + '/raw_data/Bass monophon/Samples/Chorus'\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(audio_dir):\n",
    "\n",
    "    # get the audio file\n",
    "    y_audio, sr = librosa.load(audio_dir + '/' + label, sr=None)\n",
    "\n",
    "    # create mel_spectrogram\n",
    "    mel_spec = generate_mel_spectrogram(y_audio, sr, target_shape=(128,128),\n",
    "                                        duration=2.0, normalize='minmax')\n",
    "\n",
    "    # get the target\n",
    "    label_name = label.replace('.wav', '')\n",
    "    target = df_targets[df_targets['fileID'] == label_name]['note_name'].values[0]\n",
    "\n",
    "    # add to lists\n",
    "    X.append(mel_spec)\n",
    "    y.append(target)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel : voir la correspondance classe -> entier\n",
    "#print(\"Classes (note -> id):\")\n",
    "#for note, idx in zip(le.classes_, range(len(le.classes_))):\n",
    "#    print(f\"{note} -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val, le = preprocess_for_conv2D(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e593b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(X.shape)\n",
    "print(len(y))\n",
    "print(y)\n",
    "print(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle\n",
    "input_shape = X_train.shape[1:]  # (128, 128, 1)\n",
    "num_classes = len(np.unique(y))\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Afficher le modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d564722",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor='val_loss',       # Metrique à surveiller (perte sur le jeu de validation)\n",
    "    patience=5,               # Nombre d'époques sans amélioration avant l'arrêt\n",
    "    restore_best_weights=True, # Restaure les poids du modèle à l'époque avec la meilleure performance\n",
    "    verbose=1                 # Affiche un message quand l'arrêt précoce est déclenché\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    validation_data=(X_val\n",
    "                     , y_val),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modele_lou_10-12-25_17h33.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = 'raw_data/Bass monophon/Samples/Distortion/B11-28100-4411-06241.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f585575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Charger le modèle\n",
    "model = load_model(path_DataBass + '/notebooks/lou/modele_lou_10-12-25_11h.keras')\n",
    "\n",
    "print(\"Conv2D model loaded successfully.\")\n",
    "\n",
    "# load exemple .wav\n",
    "exemple_wav = os.path.join(\n",
    "    path_DataBass,\n",
    "    'raw_data',\n",
    "    'Bass monophon',\n",
    "    'Samples',\n",
    "    'Chorus',\n",
    "    'B11-28100-3311-00625.wav')\n",
    "\n",
    "y, sr = librosa.load(exemple_wav)\n",
    "\n",
    "s = generate_mel_spectrogram(y, sr, normalize='minmax')\n",
    "\n",
    "print(s.shape)\n",
    "\n",
    "s = np.expand_dims(s, axis=-1)\n",
    "s = np.expand_dims(s, axis=0)\n",
    "\n",
    "print(f'Preprocessed shape: {s.shape}')\n",
    "\n",
    "result = model.predict(\n",
    "    s\n",
    ")\n",
    "predicted_classes = np.argmax(result, axis=1)\n",
    "\n",
    "print(\"Indices des classes prédites :\", predicted_classes)\n",
    "\n",
    "# Récupérer les notes originales\n",
    "predicted_notes = le.inverse_transform(predicted_classes)\n",
    "print(\"Notes prédites :\", predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "note = conv2D_predict_note(s, model, le)\n",
    "note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sauvegarder le LabelEncoder dans un fichier\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bcfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = pd.read_csv(path_DataBass + '/data/preprocessed/bass_list.csv')\n",
    "len(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Récupérer WORKING_DIR\n",
    "WORKING_DIR = os.getenv(\"WORKING_DIR\")\n",
    "\n",
    "# Dossier de sortie pour les spectrogrammes\n",
    "output_dir = WORKING_DIR + \"/spectrograms\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_path, mel_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd326f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the dataframe\n",
    "for index, row in sample_list.iterrows():\n",
    "\n",
    "    audio_path = row['file_path']\n",
    "\n",
    "    # check that the file exists\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"⚠️ Fichier introuvable : {audio_path}\")\n",
    "        continue\n",
    "\n",
    "    # create the Mel-spectrogramme\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        mel_spec = generate_mel_spectrogram(y, sr, normalize='minmax',\n",
    "                                            target_shape=(128,128))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du traitement de {audio_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # output folder for each note\n",
    "    note_dir = os.path.join(output_dir, row['note_name'])\n",
    "    os.makedirs(note_dir, exist_ok=True)\n",
    "\n",
    "    # output file name (.npy)\n",
    "    output_filename = f\"{row['fileID']}.npy\"\n",
    "    output_path = os.path.join(note_dir, output_filename)\n",
    "\n",
    "    # Save the spectrogram .npy\n",
    "    np.save(output_path, mel_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataBass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
